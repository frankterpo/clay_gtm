#!/usr/bin/env python3
"""
Webinar Data Processing Pipeline
===============================

Simple, powerful, straightforward processing of webinar Excel exports into Clay-ready CSV.

Usage:
    python3 process_webinar_data.py "path/to/webinar.xlsx"

Output:
    - Individual CSV files for each Excel tab (cleaned)
    - webinar_clay_import.csv (comprehensive Clay import file)
    - data_relationships.md (documentation)

Requirements: gnumeric (for Excel conversion)
Install: brew install gnumeric
"""

import os
import sys
import subprocess
from pathlib import Path

def run_command(cmd, description=""):
    """Run shell command and return success"""
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        if result.returncode != 0:
            print(f"‚ùå {description} failed: {result.stderr}")
            return False, result.stderr
        return True, result.stdout
    except Exception as e:
        print(f"‚ùå {description} error: {e}")
        return False, str(e)

def clean_csv_file(csv_path, filename, dst_path=None):
    """Clean a CSV file by removing duplicates and null BMIDs"""

    if not os.path.exists(csv_path):
        return False

    # If no destination path provided, overwrite the source
    if dst_path is None:
        dst_path = csv_path

    # Count original lines
    with open(csv_path, 'r') as f:
        original_lines = sum(1 for _ in f)

    temp_file = dst_path + '.tmp'

    if filename == 'CRM.csv':
        # CRM uses linkedin_url, no cleaning needed for now
        print(f"  {filename}: {original_lines-1} records (CRM data kept as-is)")
        return True

    # Clean using shell commands (much more reliable than pandas)
    # Remove lines where BMID is empty (assuming BMID is column 8, 1-indexed)
    cmd = f"awk -F',' 'NR==1 || $8 != \"\"' '{csv_path}' > '{temp_file}'"
    success, _ = run_command(cmd, f"Remove null BMIDs from {filename}")

    if not success:
        return False

    # For most files, remove duplicate BMIDs (keep first occurrence)
    if filename in ['attend list.csv', 'registered list.csv', 'did not attend list.csv']:
        # Remove duplicates based on BMID (column 8)
        cmd = f"awk -F',' '!seen[$8]++' '{temp_file}' > '{dst_path}'"
        success, _ = run_command(cmd, f"Remove duplicates from {filename}")
        if not success:
            return False
    elif filename == 'emoji eeaction.csv':
        # Special case: remove ALL records with duplicate BMIDs (data corruption)
        cmd = f"awk -F',' 'NR==1 || !seen[$8]++' '{temp_file}' > '{dst_path}' && awk -F',' 'seen[$8]++' '{temp_file}' | wc -l"
        success, output = run_command(cmd, f"Remove duplicate BMIDs from {filename}")
        if success and 'duplicate' in output.lower():
            print(f"  {filename}: removed duplicate BMID records")
    else:
        # For polls and Q&A, keep all records (multiple per person expected)
        cmd = f"mv '{temp_file}' '{dst_path}'"
        run_command(cmd, f"Keep all records in {filename}")

    # Count final lines
    with open(dst_path, 'r') as f:
        final_lines = sum(1 for _ in f)

    removed = original_lines - final_lines
    if removed > 0:
        print(f"  Cleaned {filename}: removed {removed} records, {final_lines-1} remaining")
    else:
        print(f"  {filename}: {final_lines-1} records (no cleaning needed)")

    # Remove temp file
    if os.path.exists(temp_file):
        os.remove(temp_file)

    return True

def create_clay_import(output_dir):
    """Create the comprehensive Clay import file by joining ALL data sources"""

    print("\nüîó Creating comprehensive Clay import file with ALL data joined...")

    clay_file = os.path.join(output_dir, 'webinar_clay_import.csv')

    # Input files (all source files are in raw_data/ with cleaning applied in-memory)
    registered_file = os.path.join('raw_data', 'registered list.csv')
    crm_file = os.path.join('raw_data', 'CRM.csv')
    attend_file = os.path.join('raw_data', 'attend list.csv')
    dna_file = os.path.join('raw_data', 'did not attend list.csv')
    polls_file = os.path.join('raw_data', 'poll responses.csv')
    emoji_file = os.path.join('raw_data', 'emoji eeaction.csv')
    qa_file = os.path.join('raw_data', 'Q&A transcript.csv')

    # Check required files
    if not os.path.exists(registered_file):
        print("‚ùå Missing registered list.csv")
        return False

    if not os.path.exists(crm_file):
        print("‚ùå Missing CRM.csv")
        return False

    # Step 1: Clean registered list (skip metadata row, remove empty BMIDs)
    print("  üìã Step 1: Cleaning registered list...")
    temp_registered = os.path.join(output_dir, 'temp_registered_clean.csv')

    try:
        import csv
        with open(registered_file, 'r', encoding='utf-8') as f_in, open(temp_registered, 'w', encoding='utf-8', newline='') as f_out:
            # Read all lines
            all_lines = f_in.readlines()
            if len(all_lines) < 3:
                print("     ERROR: Not enough lines in registered file")
                return False

            # Use csv.DictReader starting from line 17 (skip 16 metadata lines)
            # The actual header is at line 17 (0-indexed as 16)
            reader = csv.DictReader(all_lines[16:])  # Skip first 16 metadata lines

            writer = csv.DictWriter(f_out, fieldnames=reader.fieldnames, quoting=csv.QUOTE_MINIMAL)
            writer.writeheader()

            for row in reader:
                # Check if BMID exists and is not empty
                bmid = row.get('BMID', '').strip()
                if not bmid:
                    continue

                # Clean embedded newlines from ALL fields in the row
                cleaned_row = {}
                for key, value in row.items():
                    if value:
                        # Replace embedded newlines and carriage returns with spaces
                        cleaned_value = str(value).replace('\n', ' ').replace('\r', ' ').strip()
                        cleaned_row[key] = cleaned_value
                    else:
                        cleaned_row[key] = value

                # Additional cleaning: remove records with empty names or malformed LinkedIn URLs
                firstname = cleaned_row.get('Firstname', '').strip()
                lastname = cleaned_row.get('Lastname', '').strip()
                linkedin_url = cleaned_row.get('LinkedIn Profile URL', '').strip()

                # Skip records with empty names
                if not firstname or not lastname:
                    continue

                # Skip records with malformed LinkedIn URLs (just domain without profile)
                if linkedin_url in ['https://linkedin.com/in/', 'https://www.linkedin.com/in/', 'https://linkedin.com/', 'https://www.linkedin.com/']:
                    continue

                writer.writerow(cleaned_row)

        print(f"  ‚úÖ Cleaned registered list")
    except Exception as e:
        print(f"‚ùå Cleaning error: {e}")
        return False

    # Step 2: Join with CRM data using LinkedIn URL
    print("  üìã Step 2: Joining CRM data...")
    temp_crm = os.path.join(output_dir, 'temp_crm_joined.csv')
    awk_crm_join = r"""
BEGIN {FS=","; OFS=","}
NR==FNR {
    if(NR>1) {
        # CRM data: linkedin_url,first_name,last_name,company_name,domain,industry,customer_status,...
        crm[$1] = $2","$3","$4","$5","$6","$7","$8","$9","$10","$11","$12","$13","$14","$15","$16","$17
    }
    next
}
{
    if(NR==1) {
        print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,"crm_first_name","crm_last_name","crm_company","crm_domain","crm_industry","crm_customer_status","crm_created_date","crm_last_activity","crm_mrr_eur","crm_employees","crm_account_tier","crm_linkedin_url","crm_website","crm_country","crm_city","attendance_status","poll_responses","emoji_reactions","qa_questions"
    } else {
        linkedin_key = $11
        if(linkedin_key in crm) {
            split(crm[linkedin_key], crm_fields, ",")
            print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,crm_fields[1],crm_fields[2],crm_fields[3],crm_fields[4],crm_fields[5],crm_fields[6],crm_fields[7],crm_fields[8],crm_fields[9],crm_fields[10],crm_fields[11],crm_fields[12],crm_fields[13],crm_fields[14],crm_fields[15],"","","",""
        } else {
            print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,"","","","","","","","","","","","","","","","","","","",""
        }
    }
}
"""
    # Use Python for CSV processing
    try:
        import csv

        # Load CRM data
        crm_data = {}
        crm_count = 0
        with open(crm_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                linkedin_url = row.get('linkedin_url', '').strip()
                if linkedin_url:
                    # Normalize URL
                    linkedin_url = linkedin_url.replace('https://www.linkedin.com/in/', 'https://linkedin.com/in/')
                    crm_data[linkedin_url] = row
                    crm_count += 1
        print(f"     Loaded {crm_count} CRM records")

        # Process registered list
        with open(temp_registered, 'r', encoding='utf-8') as f_in, open(temp_crm, 'w', encoding='utf-8', newline='') as f_out:
            reader = csv.DictReader(f_in)

            fieldnames = list(reader.fieldnames) + [
                'crm_first_name', 'crm_last_name', 'crm_company_name', 'crm_company_domain',
                'crm_industry', 'crm_customer_status', 'crm_created_at', 'crm_last_activity_at',
                'crm_mrr_eur', 'crm_employees', 'crm_account_tier', 'attendance_status',
                'poll_responses', 'emoji_reactions', 'qa_questions'
            ]

            writer = csv.DictWriter(f_out, fieldnames=fieldnames)
            writer.writeheader()

            match_count = 0
            for row in reader:
                linkedin_key = row.get('LinkedIn Profile URL', '').strip()
                linkedin_key = linkedin_key.replace('https://www.linkedin.com/in/', 'https://linkedin.com/in/')

                if linkedin_key and linkedin_key in crm_data:
                    match_count += 1
                    crm = crm_data[linkedin_key]
                    row.update({
                        'crm_first_name': crm.get('first_name', ''),
                        'crm_last_name': crm.get('last_name', ''),
                        'crm_company_name': crm.get('company_name', ''),
                        'crm_company_domain': crm.get('company_domain', ''),
                        'crm_industry': crm.get('industry', ''),
                        'crm_customer_status': crm.get('customer_status', ''),
                        'crm_created_at': crm.get('created_at', ''),
                        'crm_last_activity_at': crm.get('last_activity_at', ''),
                        'crm_mrr_eur': crm.get('mrr_eur', ''),
                        'crm_employees': crm.get('employees', ''),
                        'crm_account_tier': crm.get('account_tier', '')
                    })
                else:
                    # Initialize activity columns
                    row['attendance_status'] = ''
                    row['poll_responses'] = 0
                    row['emoji_reactions'] = 0
                    row['qa_questions'] = 0

                # Ensure only expected fields are written
                clean_row = {k: str(row.get(k, '')).replace('\n', ' ').replace('\r', ' ').strip() for k in fieldnames}
                writer.writerow(clean_row)

        print(f"     CRM matches found: {match_count}")
        success = True
    except Exception as e:
        print(f"‚ùå CRM join error: {e}")
        success = False

    if not success:
        return False

    # Step 3: Add attendance status
    print("  üìã Step 3: Adding attendance status...")
    temp_attendance = os.path.join(output_dir, 'temp_attendance.csv')

    try:
        # Load attendance data (skip metadata row)
        attend_bmids = set()
        dna_bmids = set()

        if os.path.exists(attend_file):
            with open(attend_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                if len(lines) >= 2:
                    reader = csv.DictReader(lines[1:])  # Skip first metadata row
                    for row in reader:
                        bmid = row.get('BMID', '').strip()
                        if bmid:
                            attend_bmids.add(bmid)

        if os.path.exists(dna_file):
            with open(dna_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                if len(lines) >= 2:
                    reader = csv.DictReader(lines[1:])  # Skip first metadata row
                    for row in reader:
                        bmid = row.get('BMID', '').strip()
                        if bmid:
                            dna_bmids.add(bmid)

        # Update CRM joined file with attendance status
        with open(temp_crm, 'r', encoding='utf-8') as f_in, open(temp_attendance, 'w', encoding='utf-8', newline='') as f_out:
            reader = csv.DictReader(f_in)
            writer = csv.DictWriter(f_out, fieldnames=reader.fieldnames, quoting=csv.QUOTE_MINIMAL)
            writer.writeheader()

            for row in reader:
                bmid = row.get('BMID', '').strip()
                if bmid in attend_bmids:
                    row['attendance_status'] = 'attended'
                elif bmid in dna_bmids:
                    row['attendance_status'] = 'did_not_attend'
                else:
                    row['attendance_status'] = 'registered_only'

                clean_row = {k: str(row.get(k, '')).replace('\n', ' ').replace('\r', ' ').strip() for k in writer.fieldnames}
                writer.writerow(clean_row)

        print("  ‚úÖ Attendance status added")
    except Exception as e:
        print(f"‚ùå Attendance join error: {e}")
        return False

    # Step 4: Aggregate poll responses
    print("  üìã Step 4: Aggregating poll responses...")
    temp_polls = os.path.join(output_dir, 'temp_polls.csv')

    try:
        # Load poll data
        poll_counts = {}
        if os.path.exists(polls_file):
            with open(polls_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    bmid = row.get('BMID', '').strip()
                    if bmid:
                        poll_counts[bmid] = poll_counts.get(bmid, 0) + 1

        # Update attendance file with poll counts
        with open(temp_attendance, 'r', encoding='utf-8') as f_in, open(temp_polls, 'w', encoding='utf-8', newline='') as f_out:
            reader = csv.DictReader(f_in)
            writer = csv.DictWriter(f_out, fieldnames=reader.fieldnames, quoting=csv.QUOTE_MINIMAL)
            writer.writeheader()

            for row in reader:
                bmid = row.get('BMID', '').strip()
                row['poll_responses'] = poll_counts.get(bmid, 0)
                clean_row = {k: str(row.get(k, '')).replace('\n', ' ').replace('\r', ' ').strip() for k in writer.fieldnames}
                writer.writerow(clean_row)

        print("  ‚úÖ Poll responses aggregated")
    except Exception as e:
        print(f"‚ùå Poll aggregation error: {e}")
        return False

    # Step 5: Aggregate emoji reactions
    print("  üìã Step 5: Aggregating emoji reactions...")
    temp_emoji = os.path.join(output_dir, 'temp_emoji.csv')

    try:
        # Load emoji data
        emoji_totals = {}
        if os.path.exists(emoji_file):
            with open(emoji_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    bmid = row.get('BMID', '').strip()
                    if bmid:
                        # Sum all emoji columns
                        emoji_cols = [col for col in row.keys() if col not in ['#', 'First Name', 'Last Name', 'BMID']]
                        emoji_sum = sum(int(row.get(col, 0) or 0) for col in emoji_cols)
                        emoji_totals[bmid] = emoji_totals.get(bmid, 0) + emoji_sum

        # Update polls file with emoji counts
        with open(temp_polls, 'r', encoding='utf-8') as f_in, open(temp_emoji, 'w', encoding='utf-8', newline='') as f_out:
            reader = csv.DictReader(f_in)
            writer = csv.DictWriter(f_out, fieldnames=reader.fieldnames, quoting=csv.QUOTE_MINIMAL)
            writer.writeheader()

            for row in reader:
                bmid = row.get('BMID', '').strip()
                row['emoji_reactions'] = emoji_totals.get(bmid, 0)
                clean_row = {k: str(row.get(k, '')).replace('\n', ' ').replace('\r', ' ').strip() for k in writer.fieldnames}
                writer.writerow(clean_row)

        print("  ‚úÖ Emoji reactions aggregated")
    except Exception as e:
        print(f"‚ùå Emoji aggregation error: {e}")
        return False

    # Step 6: Aggregate Q&A questions
    print("  üìã Step 6: Aggregating Q&A questions...")
    temp_qa = os.path.join(output_dir, 'temp_qa.csv')

    try:
        # Load Q&A data
        qa_counts = {}
        if os.path.exists(qa_file):
            with open(qa_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    bmid = row.get('BMID', '').strip()
                    if bmid:
                        qa_counts[bmid] = qa_counts.get(bmid, 0) + 1

        # Update emoji file with Q&A counts
        with open(temp_emoji, 'r', encoding='utf-8') as f_in, open(temp_qa, 'w', encoding='utf-8', newline='') as f_out:
            reader = csv.DictReader(f_in)
            writer = csv.DictWriter(f_out, fieldnames=reader.fieldnames, quoting=csv.QUOTE_MINIMAL)
            writer.writeheader()

            for row in reader:
                bmid = row.get('BMID', '').strip()
                row['qa_questions'] = qa_counts.get(bmid, 0)
                clean_row = {k: str(row.get(k, '')).replace('\n', ' ').replace('\r', ' ').strip() for k in writer.fieldnames}
                writer.writerow(clean_row)

        print("  ‚úÖ Q&A questions aggregated")
    except Exception as e:
        print(f"‚ùå Q&A aggregation error: {e}")
        return False

    # Step 7: Clean URLs and finalize
    print("  üìã Step 7: Final cleanup and URL cleaning...")
    temp_final = os.path.join(output_dir, 'temp_final.csv')
    cmd = f"awk -F',' 'BEGIN{{OFS=\",\"}} {{if(NR==1) print $0; else {{$11 = ($11 == \"https://linkedin.com/in/\" || $11 == \"https://www.linkedin.com/in/\") ? \"\" : $11; $22 = ($22 == \"https://linkedin.com/in/\" || $22 == \"https://www.linkedin.com/in/\") ? \"\" : $22; print $0}}}}' '{temp_qa}' > '{temp_final}'"
    success, _ = run_command(cmd, "Clean incomplete LinkedIn URLs")

    if success:
        cmd = f"mv '{temp_final}' '{clay_file}'"
        run_command(cmd, "Create final comprehensive Clay import file")
    else:
        cmd = f"mv '{temp_qa}' '{clay_file}'"
        run_command(cmd, "Create final Clay import file (without URL cleaning)")

        # Clean up temp files
        temp_files = [temp_registered, temp_crm, temp_attendance, temp_polls, temp_emoji, temp_qa]
        for temp_file in temp_files:
            try:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
            except:
                pass  # Ignore cleanup errors

    # Count final statistics
    crm_matches = 0
    attended_count = 0
    dna_count = 0
    poll_participants = 0
    emoji_participants = 0
    qa_participants = 0
    total_records = 0

    with open(clay_file, 'r') as f:
        for line_num, line in enumerate(f, 1):
            if line_num == 1:
                continue
            total_records += 1
            fields = line.strip().split(',')

            # CRM match (column 17: crm_customer_status)
            if len(fields) > 16 and fields[16].strip():
                crm_matches += 1

            # Attendance status (column 26)
            if len(fields) > 25:
                if fields[25] == 'attended':
                    attended_count += 1
                elif fields[25] == 'did_not_attend':
                    dna_count += 1

            # Activity counts
            if len(fields) > 26 and fields[26] and fields[26] != '0':
                poll_participants += 1
            if len(fields) > 27 and fields[27] and fields[27] != '0':
                emoji_participants += 1
            if len(fields) > 28 and fields[28] and fields[28] != '0':
                qa_participants += 1

    print("  ‚úÖ Created comprehensive Clay import file with ALL data joined")
    print(f"     Total records: {total_records}")
    print(f"     CRM enriched: {crm_matches} ({crm_matches/total_records*100:.1f}%)")
    print(f"     Attended: {attended_count} ({attended_count/total_records*100:.1f}%)")
    print(f"     Did not attend: {dna_count} ({dna_count/total_records*100:.1f}%)")
    print(f"     Poll participants: {poll_participants} ({poll_participants/total_records*100:.1f}%)")
    print(f"     Emoji reactors: {emoji_participants} ({emoji_participants/total_records*100:.1f}%)")
    print(f"     Q&A askers: {qa_participants} ({qa_participants/total_records*100:.1f}%)")

    return True

def process_excel_file(excel_path):
    """Process Excel file into CSVs using ssconvert"""

    if not os.path.exists(excel_path):
        print(f"‚ùå Excel file not found: {excel_path}")
        return False

    # Create timestamped processing directory
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    processing_dir = f"processed_{timestamp}"

    # Ensure raw_data directory exists
    os.makedirs("raw_data", exist_ok=True)
    os.makedirs(processing_dir, exist_ok=True)

    print(f"üöÄ Processing webinar data from: {excel_path}")
    print(f"üìÅ Processing directory: {processing_dir}")
    print(f"üéØ Final output: raw_data/webinar_clay_import.csv")

    # Check if ssconvert is available
    success, _ = run_command("which ssconvert", "Check ssconvert availability")
    if not success:
        print("‚ùå ssconvert not found. Install with: brew install gnumeric")
        return False

    # Convert Excel to individual CSV files in raw_data/
    print("\nüìä Converting Excel tabs to CSV...")
    cmd = f"ssconvert -S '{excel_path}' 'raw_data/%s.csv'"
    success, output = run_command(cmd, "Convert Excel to CSV")

    if not success:
        print(f"‚ùå Excel conversion failed")
        return False

    # List created files
    csv_files = [f for f in os.listdir("raw_data") if f.endswith('.csv') and f != 'webinar_clay_import.csv']
    print(f"   Created {len(csv_files)} CSV files in raw_data/: {', '.join(csv_files)}")

    # Note: Source files remain in raw_data/, cleaning is handled in create_clay_import

    # Create Clay import file
    create_clay_import(processing_dir)

    # Create documentation
    create_documentation(processing_dir)

    # Final result stays only in processed folder
    clay_file_final = os.path.join(processing_dir, 'webinar_clay_import.csv')

    print("\nüéâ Processing complete!")
    print(f"   üìÇ Processing files saved to: {processing_dir}")
    print(f"   üéØ Final Clay import: {clay_file_final}")

    return True

def create_documentation(output_dir):
    """Create documentation file"""

    from datetime import datetime
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    doc_content = f"""# Webinar Data Processing Results

## Processing Summary
- **Processed**: {timestamp}
- **Status**: ‚úÖ Complete
- **Method**: Shell-based processing (reliable, no dependencies)

## Files Created
- Individual CSV files for each Excel tab (cleaned and deduplicated)
- `webinar_clay_import.csv` - Ready for Clay import
- This documentation file

## Data Cleaning Applied
- Removed records with missing BMID identifiers
- Deduplicated attendance and registration data
- Cleaned emoji data corruption issues
- Preserved multiple responses per person (polls, Q&A)

## Clay Import Ready
The `webinar_clay_import.csv` file contains:
- All registrant data with attendance status
- CRM enrichment (if CRM tab was present)
- Basic structure ready for Clay automations

## Next Steps
1. Import `webinar_clay_import.csv` into Clay
2. Set BMID as primary key for deduplication
3. Configure LinkedIn URL enrichment for social data
4. Set up lead scoring based on attendance and engagement
5. Add poll/emoji/Q&A aggregations if needed for advanced analysis

## Reprocessing Future Webinars
Run: `python3 process_webinar_data.py "new_webinar.xlsx"`
"""

    doc_file = os.path.join(output_dir, 'data_relationships.md')
    with open(doc_file, 'w') as f:
        f.write(doc_content)

    print(f"  ‚úÖ Created documentation")

def main():
    """Main entry point"""

    if len(sys.argv) != 2:
        print("üéØ Webinar Data Processing Pipeline")
        print("=" * 40)
        print("Simple, powerful, straightforward!")
        print()
        print("Usage:")
        print('  python3 process_webinar_data.py "path/to/webinar.xlsx"')
        print()
        print("Output:")
        print("  - Individual CSV files for each Excel tab (cleaned)")
        print("  - webinar_clay_import.csv (Clay-ready import file)")
        print("  - data_relationships.md (documentation)")
        print()
        print("Requirements:")
        print("  - gnumeric: brew install gnumeric")
        print()
        print("Example:")
        print('  python3 process_webinar_data.py "GTM Webinar Export.xlsx"')
        sys.exit(1)

    excel_path = sys.argv[1]
    success = process_excel_file(excel_path)

    if success:
        print("\n‚úÖ Webinar processing complete! Ready for Clay import.")
    else:
        print("\n‚ùå Processing failed.")
        sys.exit(1)

if __name__ == "__main__":
    main()
